{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGkvN0Mvadxd"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LB2Q9pgs3y2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ad610b-cd13-468e-ec48-d6de8a2d7225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.6/808.6 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.4.4 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#install required packages\n",
        "!pip install -q kaleido cohere openai tiktoken transformers peft  accelerate bitsandbytes safetensors sentencepiece streamlit chromadb langchain sentence-transformers gradio pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "As3Q9fo_Aska"
      },
      "outputs": [],
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b08wbHdz3y2m"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "# import os\n",
        "# import chromadb\n",
        "# from langchain.document_loaders import TextLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuG_TItcadxm"
      },
      "source": [
        "# Data - PDF management\n",
        "#### -> Data input: Pdf\n",
        "#### -> Data(1) output: Load Documents = \"documents\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2B4l8Oradxm"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVdYNvptU5Wu",
        "outputId": "ecac3fee-b7bf-48a3-eccb-cc5058fea759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# mount google drive and specify folder path\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/MyDrive/coexpert/raw_data'\n",
        "\n",
        "# load pdf files\n",
        "loader = PyPDFDirectoryLoader(folder_path)\n",
        "documents = loader.load()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print number of documents\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "p=0\n",
        "print(f\"Document metadata: {documents[p].metadata}\")\n",
        "print(f\"Document source: {documents[p].metadata['source']}\")\n",
        "print(f\"Document page: {documents[p].metadata['page']}\")\n",
        "print(f\"Document text: {documents[p].page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xClhZkXnvvf",
        "outputId": "66866ab5-83aa-406c-caea-5f95d3d0f94e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 184 documents\n",
            "Document metadata: {'source': '/content/drive/MyDrive/coexpert/raw_data/_51114084-001A_NG4_Tachy_ICD_PTM_en_S.pdf', 'page': 0}\n",
            "Document source: /content/drive/MyDrive/coexpert/raw_data/_51114084-001A_NG4_Tachy_ICD_PTM_en_S.pdf\n",
            "Document page: 0\n",
            "Document text: PHYSICIAN’S TECHNICAL MANUAL\n",
            "RESONATE™HFICD,\n",
            "RESONATE™ELICD,\n",
            "PERCIVA™HFICD,\n",
            "PERCIVA™ICD,\n",
            "VIGILANT™ELICD,\n",
            "MOMENTUM™ELICD\n",
            "IMPLANTABLE CARDIOVERTER DEFIBRILLATOR\n",
            "REFD520,D521,D532,D533,D420,D421,D432,D433,D500,D501,D512,D513,D400,D401,\n",
            "D412,D413,D220,D221,D232,D233,D120,D121\n",
            "CAUTION: Federallaw(USA)restricts\n",
            "thisdevicetosalebyoronthe\n",
            "orderofaphysician trainedor\n",
            "experiencedindeviceimplantand\n",
            "follow-up procedures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ugLJEuRadxo"
      },
      "source": [
        "# Data Preprocessing\n",
        "#### -> Data input: Documents from Data(1) output\n",
        "#### -> Data (2) output: Text Splitting + Chunking = \"all_splits\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkycmpG0adxp"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4voo7a_padxp"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split the documents in small chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #Change the chunk_size and chunk_overlap as needed\n",
        "all_splits = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DJR\n",
        "# print number of documents\n",
        "print(f\"Loaded {len(all_splits)} documents\")\n",
        "p=0\n",
        "print(f\"Document metadata: {all_splits[p].metadata}\")\n",
        "print(f\"Document source: {all_splits[p].metadata['source']}\")\n",
        "print(f\"Document page: {all_splits[p].metadata['page']}\")\n",
        "print(f\"Document text: {all_splits[p].page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH1-PmvQsCz7",
        "outputId": "43067705-7ebe-4491-d110-72fed2246ed5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 341 documents\n",
            "Document metadata: {'source': '/content/drive/MyDrive/coexpert/raw_data/_51114084-001A_NG4_Tachy_ICD_PTM_en_S.pdf', 'page': 0}\n",
            "Document source: /content/drive/MyDrive/coexpert/raw_data/_51114084-001A_NG4_Tachy_ICD_PTM_en_S.pdf\n",
            "Document page: 0\n",
            "Document text: PHYSICIAN’S TECHNICAL MANUAL\n",
            "RESONATE™HFICD,\n",
            "RESONATE™ELICD,\n",
            "PERCIVA™HFICD,\n",
            "PERCIVA™ICD,\n",
            "VIGILANT™ELICD,\n",
            "MOMENTUM™ELICD\n",
            "IMPLANTABLE CARDIOVERTER DEFIBRILLATOR\n",
            "REFD520,D521,D532,D533,D420,D421,D432,D433,D500,D501,D512,D513,D400,D401,\n",
            "D412,D413,D220,D221,D232,D233,D120,D121\n",
            "CAUTION: Federallaw(USA)restricts\n",
            "thisdevicetosalebyoronthe\n",
            "orderofaphysician trainedor\n",
            "experiencedindeviceimplantand\n",
            "follow-up procedures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGjqkT8eadxq"
      },
      "source": [
        "# Data Embedding\n",
        "#### -> Data(3) input: Chunked documents = Data(2) output\n",
        "#### -> Chat(3) input: Text Messages _through_ \"retriever\" from \"create_conversation\"\n",
        "#### -> Data(4) output: Vectorized Chunks = \"embeddings\"\n",
        "#### -> Chat(4) output: Vectorized Messages _through_ \"retriever\" from \"create_conversation\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [https://huggingface.co/sentence-transformers/all-mpnet-base-v2]\n",
        "### This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
        "### usage:\n",
        "```\n",
        "# pip install -U sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "# embeddings = model.encode(sentences)\n",
        "# print(embeddings)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "H2Lu3pO3wsui"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvRrQ5Vxadxq"
      },
      "source": [
        "# ![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OXTQ6xisadxq"
      },
      "outputs": [],
      "source": [
        "from functools import cache\n",
        "# Initializing a pre-trained model from Hugging Face's transformers library\n",
        "# to generate sentence embeddings. Sentence embeddings are vector\n",
        "# representations of sentences that can be used for NLP tasks.\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# specify embedding model (using huggingface sentence transformer)\n",
        "# embedding_model_name is the name of a pre-trained model available in the\n",
        "# Hugging Face model hub. The model is from the sentence-transformers\n",
        "# repository and it's called all-mpnet-base-v2\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "# The \"device\": \"cuda\" pair means that the model will be loaded onto the GPU\n",
        "# if one is available.\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "# initialize embedding model with cache folder\n",
        "\n",
        "# with CUDA:\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name=embedding_model_name, model_kwargs=model_kwargs,cache_folder=\"/content/drive/MyDrive/coexpert/models/embedding_model\")\n",
        "\n",
        "# Without CUDA:\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "     model_name=embedding_model_name,cache_folder=\"/content/drive/MyDrive/coexpert/models/embedding_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClLfd8zu1o6E",
        "outputId": "a3082173-04d0-4b93-d30f-573dbd7732b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
              "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  (2): Normalize()\n",
              "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder='/content/drive/MyDrive/coexpert/models/embedding_model', model_kwargs={}, encode_kwargs={}, multi_process=False)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming embeddings is your HuggingFaceEmbeddings instance\n",
        "# and it has a property or method that returns the underlying Hugging Face model\n",
        "#model = embeddings.model\n",
        "\n",
        "# Specify the directory path where you want to save the model\n",
        "#save_directory = \"/content/drive/MyDrive/coexpert/models/embedding_model\"\n",
        "\n",
        "# Save the model\n",
        "#model.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "HwiiOzpn0s3r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMRvfVWYadxr"
      },
      "source": [
        "# Data & Chat Vector Management (Storage & Similarities)\n",
        "#### -> Data(5) input:  Vectorized Chunks (Embeddings) from Data(4) ouput + Text chunks from Data(2) output = \"vectordb\"\n",
        "#### -> Chat(5) input: Vectorized Messages _through_ \"retriever\" from \"create_conversation\"\n",
        "#### -> Chat(6) output: Similiarity Search + Retrieve Text Chunks = \"retriever\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGvLz0vmadxr"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GlWVUlP8AiHJ"
      },
      "outputs": [],
      "source": [
        "# using the Chroma class from the langchain.vectorstores module to create\n",
        "# a vector database from  all-splits documents,\n",
        "# then creating a retriever from that database\n",
        "\n",
        "# Chroma is a class that handles the creation and manipulation of a\n",
        "# vector database, which is a database designed to efficiently store and\n",
        "#retrieve vector representations of data.\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# embed document chunks\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"/content/drive/MyDrive/coexpert/models/chroma_db\")\n",
        "\n",
        "# specify the retriever\n",
        "# A retriever is an object that can efficiently find the most similar vectors in the database given a query vector.\n",
        "retriever = vectordb.as_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wq0kOHiadxs"
      },
      "source": [
        "# Chat Modelling\n",
        "#### -> Chat(7) input: Message + Retrieve Text Chunks _through_ \"create_conversation\"\n",
        "#### -> Chat(8) output: LLM Response = \"llm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PciOnAwNadxs"
      },
      "source": [
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kKkVcgAv8frU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# specify model huggingface mode name\n",
        "model_name = \"anakin87/zephyr-7b-alpha-sharded\"\n",
        "\n",
        "######## other models:\n",
        "# \"Trelis/Llama-2-7b-chat-hf-sharded-bf16\"\n",
        "# \"bn22/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "# \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# function for loading 4-bit quantized model\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2C0DVleS8iwj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# fucntion for initializing tokenizer\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OTLGihCWuv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQP-tdh88qiv"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# specify stop token ids\n",
        "stop_token_ids = [0] # where is it used?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VVuVbgj9m6Ja"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import HuggingFacePipeline # why 2 imports?\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# build huggingface pipeline for using zephyr-7b-alpha\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=2048,\n",
        "        do_sample=True,\n",
        "        top_k=5,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# specify the llm\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBJ7c5fadxt"
      },
      "source": [
        "# Chat Interface-API\n",
        "#### -> Chat(1&2) input: Message _through_ \"create_conversation\"\n",
        "#### -> Chat(9&10) output: LLM Response _through_ \"create_conversation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo-ynGo9adxt"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SVFI_rP5adxt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# build conversational retrieval chain with memory (rag) using langchain\n",
        "def create_conversation(query: str, chat_history: list) -> tuple:\n",
        "    try:\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history',\n",
        "            return_messages=False\n",
        "        )\n",
        "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory,\n",
        "            get_chat_history=lambda h: h,\n",
        "        )\n",
        "\n",
        "        result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "        chat_history.append((query, result['answer']))\n",
        "        return '', chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append((query, e))\n",
        "        return '', chat_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "dETfjAAAjuaU",
        "outputId": "2b4398b6-bcbd-4874-8293-94b9a46f6b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://93d2edba728edf13b3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93d2edba728edf13b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# build gradio ui\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    chatbot = gr.Chatbot(label='Chat with your data (Zephyr 7B Alpha)')\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    msg.submit(create_conversation, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}